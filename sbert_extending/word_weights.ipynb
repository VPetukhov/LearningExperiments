{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence_transformers torch wandb python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "sts_dataset_path = 'datasets/stsbenchmark.tsv.gz'\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/stsbenchmark.tsv.gz', sts_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvpetukhov\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20221123_163356-1ehaapex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vpetukhov/ea-forum-analysis/runs/1ehaapex\" target=\"_blank\">dutiful-sun-49</a></strong> to <a href=\"https://wandb.ai/vpetukhov/ea-forum-analysis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login() # relies on WANDB_API_KEY env var\n",
    "run = wandb.init(\n",
    "    project=\"ea-forum-analysis\", job_type=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact post_pairs:latest, 403.55MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:18.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src_text', 'dst_text', 'src_post_id', 'dst_post_id', 'sims'],\n",
       "        num_rows: 174991\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['src_text', 'dst_text', 'src_post_id', 'dst_post_id', 'sims'],\n",
       "        num_rows: 4220\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art = run.use_artifact(\"post_pairs:latest\")\n",
    "data = DatasetDict.load_from_disk(art.download())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 17:00:08 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2022-11-23 17:00:08 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "# model_name = 'nli-distilroberta-base-v2'\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "train_batch_size = 256\n",
    "num_epochs = 1\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer(model_name).cuda()\n",
    "\n",
    "train_samples = [InputExample(texts=ts, label=float(l)) for ts,l in zip(zip(data['train']['src_text'], data['train']['dst_text']), data['train']['sims'])]\n",
    "evaluator = EmbeddingSimilarityEvaluator(data['dev']['src_text'], data['dev']['dst_text'], data['dev']['sims'], batch_size=512)\n",
    "\n",
    "# train_samples = []\n",
    "# dev_samples = []\n",
    "# test_samples = []\n",
    "# with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "#     reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "#     for row in reader:\n",
    "#         score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1\n",
    "#         inp_example = InputExample(texts=[row['sentence1'], row['sentence2']], label=score)\n",
    "\n",
    "#         if row['split'] == 'dev':\n",
    "#             dev_samples.append(inp_example)\n",
    "#         elif row['split'] == 'test':\n",
    "#             test_samples.append(inp_example)\n",
    "#         else:\n",
    "#             train_samples.append(inp_example)\n",
    "\n",
    "# train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "# train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 15:00:14 - Read STSbenchmark dev dataset\n"
     ]
    }
   ],
   "source": [
    "# Development set: Measure correlation between cosine score and gold labels\n",
    "# logging.info(\"Read STSbenchmark dev dataset\")\n",
    "# evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 16:43:32 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 16:43:52 - Cosine-Similarity :\tPearson: 0.6034\tSpearman: 0.5573\n",
      "2022-11-23 16:43:52 - Manhattan-Distance:\tPearson: 0.6100\tSpearman: 0.5580\n",
      "2022-11-23 16:43:52 - Euclidean-Distance:\tPearson: 0.6106\tSpearman: 0.5573\n",
      "2022-11-23 16:43:52 - Dot-Product-Similarity:\tPearson: 0.6034\tSpearman: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5580159339745571"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested SBERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 15:01:31 - Warmup-steps: 144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e621399857e4149b3e668c24ebaaeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83329975c64428bca1c578acb028c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 15:02:12 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-dev dataset after epoch 0:\n",
      "2022-11-23 15:02:14 - Cosine-Similarity :\tPearson: 0.8882\tSpearman: 0.8889\n",
      "2022-11-23 15:02:14 - Manhattan-Distance:\tPearson: 0.8607\tSpearman: 0.8634\n",
      "2022-11-23 15:02:14 - Euclidean-Distance:\tPearson: 0.8628\tSpearman: 0.8663\n",
      "2022-11-23 15:02:14 - Dot-Product-Similarity:\tPearson: 0.8396\tSpearman: 0.8415\n",
      "2022-11-23 15:02:14 - Save model to output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-11-23_14-58-46\n",
      "2022-11-23 15:02:20 - Load pretrained SentenceTransformer: output/training_stsbenchmark_continue_training-nli-distilroberta-base-v2-2022-11-23_14-58-46\n",
      "2022-11-23 15:02:21 - Use pytorch device: cuda\n",
      "2022-11-23 15:02:21 - EmbeddingSimilarityEvaluator: Evaluating the model on sts-test dataset:\n",
      "2022-11-23 15:02:23 - Cosine-Similarity :\tPearson: 0.8589\tSpearman: 0.8582\n",
      "2022-11-23 15:02:23 - Manhattan-Distance:\tPearson: 0.8262\tSpearman: 0.8288\n",
      "2022-11-23 15:02:23 - Euclidean-Distance:\tPearson: 0.8297\tSpearman: 0.8319\n",
      "2022-11-23 15:02:23 - Dot-Product-Similarity:\tPearson: 0.8093\tSpearman: 0.8021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.858169904173178"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the training. We skip evaluation in this example\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=1,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 14:55:10 - Load pretrained SentenceTransformer: nli-distilroberta-base-v2\n",
      "2022-11-23 14:55:11 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_name)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_features, t_labs = next(iter(train_dataloader))\n",
    "pred_sims = torch.cosine_similarity(\n",
    "    model(t_features[0])['sentence_embedding'], \n",
    "    model(t_features[1])['sentence_embedding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sims = torch.cosine_similarity(\n",
    "    model[1](model[0](t_features[0]))['sentence_embedding'],\n",
    "    model[1](model[0](t_features[1]))['sentence_embedding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6c5b353bb0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pred_sims.detach().numpy(), t_labs.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New pooling class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class WPooling(nn.Module):\n",
    "    def __init__(self, word_embedding_dimension: int):\n",
    "        super(WPooling, self).__init__()\n",
    "        self.word_embedding_dimension = word_embedding_dimension\n",
    "        self.pooling_output_dimension = word_embedding_dimension\n",
    "        self.weight_layer = nn.Linear(word_embedding_dimension, 1, bias=True)\n",
    "        self.sigma_layer = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, features: Dict[str, Tensor]):\n",
    "        token_embeddings = features['token_embeddings']\n",
    "        attention_mask = features['attention_mask']\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "        masked_embeddings = (token_embeddings * input_mask_expanded)\n",
    "        token_weights = self.sigma_layer(self.weight_layer(masked_embeddings))\n",
    "        # token_weights = torch.zeros_like(masked_embeddings) + 1\n",
    "\n",
    "        weighted_masked_embeddings = token_weights * masked_embeddings\n",
    "\n",
    "        sum_embeddings = torch.sum(weighted_masked_embeddings, 1)\n",
    "\n",
    "        sum_mask = (token_weights * input_mask_expanded).sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "        output_vector = sum_embeddings / sum_mask\n",
    "        features.update({\n",
    "            'sentence_embedding': output_vector, 'token_weights': token_weights,\n",
    "            'masked_embeddings': masked_embeddings, 'input_mask_expanded': input_mask_expanded,\n",
    "            'sum_embeddings': sum_embeddings, 'sum_mask': sum_mask, 'weighted_masked_embeddings': weighted_masked_embeddings\n",
    "        })\n",
    "        return features\n",
    "\n",
    "    # def forward(self, features: Dict[str, Tensor]):\n",
    "    #     token_embeddings = features['token_embeddings']\n",
    "    #     attention_mask = features['attention_mask']\n",
    "\n",
    "    #     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    #     sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "\n",
    "    #     sum_mask = input_mask_expanded.sum(1)\n",
    "    #     sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "    #     output_vector = sum_embeddings / sum_mask\n",
    "    #     features.update({'sentence_embedding': output_vector})\n",
    "    #     return features\n",
    "\n",
    "    def get_sentence_embedding_dimension(self):\n",
    "        return self.pooling_output_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = model_weighted(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr['token_weights'][1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr['weighted_masked_embeddings'][1, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr['masked_embeddings'][1, 2, :] * tr['token_weights'][1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:02:53 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2022-11-23 18:02:53 - Use pytorch device: cuda\n",
      "2022-11-23 18:02:53 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_name)\n",
    "model_raw = SentenceTransformer(\n",
    "    modules = [model[0], WPooling(model[0].get_word_embedding_dimension())]\n",
    ").cuda()\n",
    "\n",
    "for p in model_raw[0].parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:03:17 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:03:37 - Cosine-Similarity :\tPearson: 0.4582\tSpearman: 0.4289\n",
      "2022-11-23 18:03:37 - Manhattan-Distance:\tPearson: 0.2996\tSpearman: 0.3261\n",
      "2022-11-23 18:03:37 - Euclidean-Distance:\tPearson: 0.2966\tSpearman: 0.3244\n",
      "2022-11-23 18:03:37 - Dot-Product-Similarity:\tPearson: 0.3580\tSpearman: 0.3797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.428904590376042"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(model_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "2022-11-23 18:03:17 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
    "2022-11-23 18:03:37 - Cosine-Similarity :\tPearson: 0.4582\tSpearman: 0.4289\n",
    "2022-11-23 18:03:37 - Manhattan-Distance:\tPearson: 0.2996\tSpearman: 0.3261\n",
    "2022-11-23 18:03:37 - Euclidean-Distance:\tPearson: 0.2966\tSpearman: 0.3244\n",
    "2022-11-23 18:03:37 - Dot-Product-Similarity:\tPearson: 0.3580\tSpearman: 0.3797\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 17:52:40 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 17:52:59 - Cosine-Similarity :\tPearson: 0.6034\tSpearman: 0.5573\n",
      "2022-11-23 17:52:59 - Manhattan-Distance:\tPearson: 0.6100\tSpearman: 0.5580\n",
      "2022-11-23 17:52:59 - Euclidean-Distance:\tPearson: 0.6106\tSpearman: 0.5573\n",
      "2022-11-23 17:52:59 - Dot-Product-Similarity:\tPearson: 0.6034\tSpearman: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5580159339745571"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "train_dataloader.collate_fn = model_raw.smart_batching_collate\n",
    "# train_loss_w = losses.CosineSimilarityLoss(model=model_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_similarity(output):\n",
    "    embeddings = [r['sentence_embedding'] for r in output]\n",
    "    return torch.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profile training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader2 = DataLoader([(t.texts, np.float32(t.label)) for t in train_samples], shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_weighted[0].tokenize([x.texts for x in train_samples[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total: 24s\n",
    "- Tokenize: 3s\n",
    "- Forward pass + Tokenize: 24s\n",
    "- BERT pass + Tokenize: 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5cf1a996c848d9aafae29399cb37ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model_weighted[1].weight_layer.parameters(), lr=1e-3)\n",
    "c_loss = nn.MSELoss()\n",
    "\n",
    "for batch, (X, y) in enumerate(tqdm(train_loader2)):\n",
    "\n",
    "        X = [{k: v.to('cuda') for k,v in model_weighted[0].tokenize(t).items()} for t in X]\n",
    "        y = y.to('cuda')\n",
    "\n",
    "        res = [model_weighted[0](sentence_feature) for sentence_feature in X]\n",
    "\n",
    "        # sim = emb_similarity(res)\n",
    "\n",
    "        # # sparsity_loss = (res[0]['token_weights'] / res[0]['token_weights'].max()).sum()# + res[1]['token_weights'].sum()\n",
    "\n",
    "        # loss = c_loss(sim, y.view(-1))# + sparsity_loss * 1e-4\n",
    "\n",
    "        # # loss = train_loss_w(X, y)\n",
    "\n",
    "        # # Backpropagation\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        if batch > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Source Location                                            \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "                                  cudaStreamSynchronize        77.59%     604.146ms        77.59%     604.146ms      86.307ms           0 b           0 b           0 b           0 b             7                                                             \n",
      "                                                                                                                                                                                                                                                                \n",
      "                                               TOKENIZE        17.91%     139.469ms        95.74%     745.453ms     745.453ms          -4 b      -3.00 Mb       3.00 Mb           0 b             1  runpy.py(87): _run_code                                    \n",
      "                                                                                                                                                                                                     ipykernel_launcher.py(17): <module>                        \n",
      "                                                                                                                                                                                                     traitlets/config/application.py(976): launch_instance      \n",
      "                                                                                                                                                                                                     ipykernel/kernelapp.py(712): start                         \n",
      "                                                                                                                                                                                                     tornado/platform/asyncio.py(215): start                    \n",
      "                                                                                                                                                                                                                                                                \n",
      "                                                   PASS         1.20%       9.370ms         3.12%      24.299ms      24.299ms          -4 b        -268 b     383.25 Mb     -58.51 Gb             1  runpy.py(87): _run_code                                    \n",
      "                                                                                                                                                                                                     ipykernel_launcher.py(17): <module>                        \n",
      "                                                                                                                                                                                                     traitlets/config/application.py(976): launch_instance      \n",
      "                                                                                                                                                                                                     ipykernel/kernelapp.py(712): start                         \n",
      "                                                                                                                                                                                                     tornado/platform/asyncio.py(215): start                    \n",
      "                                                                                                                                                                                                                                                                \n",
      "                                               BACKPROP         0.43%       3.367ms         0.48%       3.755ms       3.755ms          -4 b        -268 b    -387.00 Mb    -387.00 Mb             1  runpy.py(87): _run_code                                    \n",
      "                                                                                                                                                                                                     ipykernel_launcher.py(17): <module>                        \n",
      "                                                                                                                                                                                                     traitlets/config/application.py(976): launch_instance      \n",
      "                                                                                                                                                                                                     ipykernel/kernelapp.py(712): start                         \n",
      "                                                                                                                                                                                                     tornado/platform/asyncio.py(215): start                    \n",
      "                                                                                                                                                                                                                                                                \n",
      "                                            aten::addmm         0.40%       3.133ms         0.55%       4.321ms      56.855us           0 b           0 b      10.13 Gb      10.12 Gb            76  runpy.py(87): _run_code                                    \n",
      "                                                                                                                                                                                                     ipykernel_launcher.py(17): <module>                        \n",
      "                                                                                                                                                                                                     traitlets/config/application.py(976): launch_instance      \n",
      "                                                                                                                                                                                                     ipykernel/kernelapp.py(712): start                         \n",
      "                                                                                                                                                                                                     tornado/platform/asyncio.py(215): start                    \n",
      "                                                                                                                                                                                                                                                                \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------  \n",
      "Self CPU time total: 778.632ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model_weighted = copy.deepcopy(model_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87766d2c4b204418ba7e047f15aac8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:08:40 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:09:00 - Cosine-Similarity :\tPearson: 0.5974\tSpearman: 0.5507\n",
      "2022-11-23 18:09:00 - Manhattan-Distance:\tPearson: 0.4088\tSpearman: 0.4407\n",
      "2022-11-23 18:09:00 - Euclidean-Distance:\tPearson: 0.4045\tSpearman: 0.4395\n",
      "2022-11-23 18:09:00 - Dot-Product-Similarity:\tPearson: 0.5464\tSpearman: 0.5167\n",
      "2022-11-23 18:10:02 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:10:22 - Cosine-Similarity :\tPearson: 0.5995\tSpearman: 0.5586\n",
      "2022-11-23 18:10:22 - Manhattan-Distance:\tPearson: 0.4158\tSpearman: 0.4410\n",
      "2022-11-23 18:10:22 - Euclidean-Distance:\tPearson: 0.3933\tSpearman: 0.4391\n",
      "2022-11-23 18:10:22 - Dot-Product-Similarity:\tPearson: 0.5468\tSpearman: 0.5250\n",
      "2022-11-23 18:11:24 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:11:44 - Cosine-Similarity :\tPearson: 0.6012\tSpearman: 0.5630\n",
      "2022-11-23 18:11:44 - Manhattan-Distance:\tPearson: 0.4170\tSpearman: 0.4401\n",
      "2022-11-23 18:11:44 - Euclidean-Distance:\tPearson: 0.3886\tSpearman: 0.4374\n",
      "2022-11-23 18:11:44 - Dot-Product-Similarity:\tPearson: 0.5301\tSpearman: 0.5184\n",
      "2022-11-23 18:12:46 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:13:06 - Cosine-Similarity :\tPearson: 0.6152\tSpearman: 0.5824\n",
      "2022-11-23 18:13:06 - Manhattan-Distance:\tPearson: 0.4091\tSpearman: 0.4185\n",
      "2022-11-23 18:13:06 - Euclidean-Distance:\tPearson: 0.3742\tSpearman: 0.4137\n",
      "2022-11-23 18:13:06 - Dot-Product-Similarity:\tPearson: 0.5265\tSpearman: 0.5245\n",
      "2022-11-23 18:14:08 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:14:28 - Cosine-Similarity :\tPearson: 0.6168\tSpearman: 0.5818\n",
      "2022-11-23 18:14:28 - Manhattan-Distance:\tPearson: 0.3930\tSpearman: 0.3890\n",
      "2022-11-23 18:14:28 - Euclidean-Distance:\tPearson: 0.3542\tSpearman: 0.3795\n",
      "2022-11-23 18:14:28 - Dot-Product-Similarity:\tPearson: 0.5057\tSpearman: 0.5134\n",
      "2022-11-23 18:15:30 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:15:51 - Cosine-Similarity :\tPearson: 0.6245\tSpearman: 0.5888\n",
      "2022-11-23 18:15:51 - Manhattan-Distance:\tPearson: 0.3913\tSpearman: 0.3858\n",
      "2022-11-23 18:15:51 - Euclidean-Distance:\tPearson: 0.3515\tSpearman: 0.3753\n",
      "2022-11-23 18:15:51 - Dot-Product-Similarity:\tPearson: 0.4965\tSpearman: 0.5158\n",
      "2022-11-23 18:16:53 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:17:13 - Cosine-Similarity :\tPearson: 0.6205\tSpearman: 0.5864\n",
      "2022-11-23 18:17:13 - Manhattan-Distance:\tPearson: 0.3787\tSpearman: 0.3792\n",
      "2022-11-23 18:17:13 - Euclidean-Distance:\tPearson: 0.3258\tSpearman: 0.3592\n",
      "2022-11-23 18:17:13 - Dot-Product-Similarity:\tPearson: 0.4445\tSpearman: 0.4978\n",
      "2022-11-23 18:18:15 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:18:35 - Cosine-Similarity :\tPearson: 0.6255\tSpearman: 0.5883\n",
      "2022-11-23 18:18:35 - Manhattan-Distance:\tPearson: 0.4014\tSpearman: 0.3980\n",
      "2022-11-23 18:18:35 - Euclidean-Distance:\tPearson: 0.3521\tSpearman: 0.3809\n",
      "2022-11-23 18:18:35 - Dot-Product-Similarity:\tPearson: 0.4929\tSpearman: 0.5253\n",
      "2022-11-23 18:19:37 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:19:57 - Cosine-Similarity :\tPearson: 0.6232\tSpearman: 0.5890\n",
      "2022-11-23 18:19:57 - Manhattan-Distance:\tPearson: 0.3782\tSpearman: 0.3769\n",
      "2022-11-23 18:19:57 - Euclidean-Distance:\tPearson: 0.3271\tSpearman: 0.3581\n",
      "2022-11-23 18:19:57 - Dot-Product-Similarity:\tPearson: 0.4527\tSpearman: 0.5082\n",
      "2022-11-23 18:20:59 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:21:19 - Cosine-Similarity :\tPearson: 0.6280\tSpearman: 0.5918\n",
      "2022-11-23 18:21:19 - Manhattan-Distance:\tPearson: 0.3897\tSpearman: 0.3872\n",
      "2022-11-23 18:21:19 - Euclidean-Distance:\tPearson: 0.3359\tSpearman: 0.3678\n",
      "2022-11-23 18:21:19 - Dot-Product-Similarity:\tPearson: 0.4485\tSpearman: 0.5002\n",
      "2022-11-23 18:22:21 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:22:41 - Cosine-Similarity :\tPearson: 0.6228\tSpearman: 0.5862\n",
      "2022-11-23 18:22:41 - Manhattan-Distance:\tPearson: 0.3868\tSpearman: 0.3834\n",
      "2022-11-23 18:22:41 - Euclidean-Distance:\tPearson: 0.3379\tSpearman: 0.3667\n",
      "2022-11-23 18:22:41 - Dot-Product-Similarity:\tPearson: 0.4484\tSpearman: 0.5087\n",
      "2022-11-23 18:23:43 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:24:04 - Cosine-Similarity :\tPearson: 0.6277\tSpearman: 0.5920\n",
      "2022-11-23 18:24:04 - Manhattan-Distance:\tPearson: 0.3939\tSpearman: 0.3894\n",
      "2022-11-23 18:24:04 - Euclidean-Distance:\tPearson: 0.3412\tSpearman: 0.3726\n",
      "2022-11-23 18:24:04 - Dot-Product-Similarity:\tPearson: 0.4633\tSpearman: 0.5176\n",
      "2022-11-23 18:25:06 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:25:26 - Cosine-Similarity :\tPearson: 0.6302\tSpearman: 0.5954\n",
      "2022-11-23 18:25:26 - Manhattan-Distance:\tPearson: 0.3878\tSpearman: 0.3815\n",
      "2022-11-23 18:25:26 - Euclidean-Distance:\tPearson: 0.3354\tSpearman: 0.3619\n",
      "2022-11-23 18:25:26 - Dot-Product-Similarity:\tPearson: 0.4545\tSpearman: 0.5126\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.SGD(model_weighted[1].weight_layer.parameters(), lr=1e-1)\n",
    "optimizer = torch.optim.Adam(model_weighted[1].weight_layer.parameters(), lr=0.05)\n",
    "\n",
    "c_loss = nn.MSELoss()\n",
    "\n",
    "for batch, (X, y) in enumerate(tqdm(train_dataloader)):\n",
    "    X = [{k: v.to('cuda') for k,v in fs.items()} for fs in X]\n",
    "    y = y.cuda()\n",
    "\n",
    "    res = [model_weighted(sentence_feature) for sentence_feature in X]\n",
    "    sim = emb_similarity(res)\n",
    "\n",
    "    # sparsity_loss = (res[0]['token_weights'] / res[0]['token_weights'].max()).sum()# + res[1]['token_weights'].sum()\n",
    "\n",
    "    loss = c_loss(sim, y.view(-1))# + sparsity_loss * 1e-4\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (batch > 0) and (batch % 100 == 0):\n",
    "        # break\n",
    "        evaluator(model_weighted)\n",
    "    \n",
    "    # if batch >= 200:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:26:31 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:26:50 - Cosine-Similarity :\tPearson: 0.6306\tSpearman: 0.5948\n",
      "2022-11-23 18:26:50 - Manhattan-Distance:\tPearson: 0.3880\tSpearman: 0.3861\n",
      "2022-11-23 18:26:50 - Euclidean-Distance:\tPearson: 0.3330\tSpearman: 0.3672\n",
      "2022-11-23 18:26:50 - Dot-Product-Similarity:\tPearson: 0.4693\tSpearman: 0.5176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5948078534110832"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(model_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 18:26:51 - EmbeddingSimilarityEvaluator: Evaluating the model on  dataset:\n",
      "2022-11-23 18:27:10 - Cosine-Similarity :\tPearson: 0.6034\tSpearman: 0.5573\n",
      "2022-11-23 18:27:10 - Manhattan-Distance:\tPearson: 0.6100\tSpearman: 0.5580\n",
      "2022-11-23 18:27:10 - Euclidean-Distance:\tPearson: 0.6106\tSpearman: 0.5573\n",
      "2022-11-23 18:27:10 - Dot-Product-Similarity:\tPearson: 0.6034\tSpearman: 0.5573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5580159339745571"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://nqzjdc1kmy.clg07azjl.paperspacegradient.com/'. Verify the server is running and reachable. (Invalid response: 503 Service Unavailable)."
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_words_per_text = np.array([[len(t.split()) for t in ts.texts] for ts in train_samples])\n",
    "plt.hist(n_words_per_text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_ids = np.where(n_words_per_text.max(axis=1) > 30)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_ex = train_samples[long_ids[0]].texts\n",
    "# inps = [model_weighted.tokenize(t) for t in t_ex]\n",
    "# preds = [model_weighted({k: v.to('cuda') for k,v in inp.items()}) for inp in inps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8304498195648193, 0.8400000000000001)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_ex = train_samples[long_ids[2]]\n",
    "inps = [model_weighted[0].tokenizer.encode_plus(t, return_tensors='pt').to('cuda') for t in t_ex.texts]\n",
    "preds = [model_weighted(inp) for inp in inps]\n",
    "emb_similarity(preds)[0].item(), t_ex.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>0.004005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r</td>\n",
       "      <td>0.008981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ussia</td>\n",
       "      <td>0.047740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ratified</td>\n",
       "      <td>0.012617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>0.001319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>updated</td>\n",
       "      <td>0.007120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>treaty</td>\n",
       "      <td>0.002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>0.006770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2004</td>\n",
       "      <td>0.013473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>but</td>\n",
       "      <td>0.016851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>0.005193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>united</td>\n",
       "      <td>0.323574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>states</td>\n",
       "      <td>0.024524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>0.015972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>other</td>\n",
       "      <td>0.017055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n</td>\n",
       "      <td>0.024225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ato</td>\n",
       "      <td>0.155510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>members</td>\n",
       "      <td>0.015711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>have</td>\n",
       "      <td>0.061872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>refused</td>\n",
       "      <td>0.644233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to</td>\n",
       "      <td>0.330967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>do</td>\n",
       "      <td>0.344332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>so</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>arguing</td>\n",
       "      <td>0.027734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>that</td>\n",
       "      <td>0.006024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mos</td>\n",
       "      <td>0.041082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.093409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>must</td>\n",
       "      <td>0.032979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>first</td>\n",
       "      <td>0.065523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fulfill</td>\n",
       "      <td>0.031258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>obligations</td>\n",
       "      <td>0.020983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>to</td>\n",
       "      <td>0.005116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>withdraw</td>\n",
       "      <td>0.761134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>forces</td>\n",
       "      <td>0.011348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>from</td>\n",
       "      <td>0.017254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ge</td>\n",
       "      <td>0.058367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>org</td>\n",
       "      <td>0.172863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ia</td>\n",
       "      <td>0.033892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>and</td>\n",
       "      <td>0.009079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>from</td>\n",
       "      <td>0.024990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>mold</td>\n",
       "      <td>0.108117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ova</td>\n",
       "      <td>0.099091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>'s</td>\n",
       "      <td>0.004443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>separatist</td>\n",
       "      <td>0.023466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>region</td>\n",
       "      <td>0.023655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>of</td>\n",
       "      <td>0.002485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>trans</td>\n",
       "      <td>0.008653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-</td>\n",
       "      <td>0.002883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>dn</td>\n",
       "      <td>0.058821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ies</td>\n",
       "      <td>0.059291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ter</td>\n",
       "      <td>0.032461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>.</td>\n",
       "      <td>0.004307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td></td>\n",
       "      <td>0.025331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token    weight\n",
       "0            <s>  0.004005\n",
       "1              r  0.008981\n",
       "2          ussia  0.047740\n",
       "3      ratified  0.012617\n",
       "4           the  0.001319\n",
       "5       updated  0.007120\n",
       "6        treaty  0.002397\n",
       "7            in  0.006770\n",
       "8          2004  0.013473\n",
       "9           but  0.016851\n",
       "10          the  0.005193\n",
       "11       united  0.323574\n",
       "12       states  0.024524\n",
       "13          and  0.015972\n",
       "14        other  0.017055\n",
       "15            n  0.024225\n",
       "16           ato  0.155510\n",
       "17      members  0.015711\n",
       "18         have  0.061872\n",
       "19      refused  0.644233\n",
       "20           to  0.330967\n",
       "21           do  0.344332\n",
       "22           so  1.000000\n",
       "23      arguing  0.027734\n",
       "24         that  0.006024\n",
       "25          mos  0.041082\n",
       "26           cow  0.093409\n",
       "27         must  0.032979\n",
       "28        first  0.065523\n",
       "29      fulfill  0.031258\n",
       "30  obligations  0.020983\n",
       "31           to  0.005116\n",
       "32     withdraw  0.761134\n",
       "33       forces  0.011348\n",
       "34         from  0.017254\n",
       "35           ge  0.058367\n",
       "36           org  0.172863\n",
       "37            ia  0.033892\n",
       "38          and  0.009079\n",
       "39         from  0.024990\n",
       "40         mold  0.108117\n",
       "41           ova  0.099091\n",
       "42            's  0.004443\n",
       "43   separatist  0.023466\n",
       "44       region  0.023655\n",
       "45           of  0.002485\n",
       "46        trans  0.008653\n",
       "47             -  0.002883\n",
       "48            dn  0.058821\n",
       "49           ies  0.059291\n",
       "50           ter  0.032461\n",
       "51             .  0.004307\n",
       "52               0.025331\n",
       "53          </s>  0.002484"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = preds[0]['token_weights'].squeeze().detach().cpu().numpy()\n",
    "DataFrame(dict(\n",
    "    token=model_weighted[0].tokenizer.tokenize(t_ex.texts[0], add_special_tokens=True), \n",
    "    weight=weights / weights.max()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert {'input_ids': [[0, 506, 2], [0, 257, 2], [0, 462, 2], [0, 705, 2], [0, 118, 2], [0, 139, 2], [0, 2, 1], [0, 428, 2], [0, 242, 2], [0, 338, 2], [0, 571, 2], [0, 298, 2], [0, 242, 2], [0, 462, 2], [0, 462, 2], [0, 102, 2], [0, 2, 1], [0, 29, 2], [0, 90, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 102, 2], [0, 90, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 242, 2], [0, 2, 1], [0, 417, 2], [0, 118, 2], [0, 571, 2], [0, 118, 2], [0, 90, 2], [0, 102, 2], [0, 462, 2], [0, 2, 1], [0, 605, 2], [0, 139, 2], [0, 338, 2], [0, 119, 2], [0, 2, 1], [0, 282, 2], [0, 102, 2], [0, 119, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 29, 2], [0, 1343, 2], [0, 2, 1], [0, 298, 2], [0, 242, 2], [0, 462, 2], [0, 462, 2], [0, 2, 1], [0, 605, 2], [0, 102, 2], [0, 29, 2], [0, 2, 1], [0, 642, 2], [0, 102, 2], [0, 338, 2], [0, 90, 2], [0, 118, 2], [0, 438, 2], [0, 257, 2], [0, 462, 2], [0, 102, 2], [0, 338, 2], [0, 462, 2], [0, 219, 2], [0, 2, 1], [0, 705, 2], [0, 118, 2], [0, 338, 2], [0, 257, 2], [0, 462, 2], [0, 242, 2], [0, 282, 2], [0, 90, 2], [0, 2, 1], [0, 102, 2], [0, 282, 2], [0, 417, 2], [0, 2, 1], [0, 338, 2], [0, 242, 2], [0, 642, 2], [0, 462, 2], [0, 118, 2], [0, 438, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 118, 2], [0, 90, 2], [0, 29, 2], [0, 242, 2], [0, 462, 2], [0, 506, 2], [0, 2, 1], [0, 102, 2], [0, 90, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 242, 2], [0, 2, 1], [0, 338, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 2, 1], [0, 139, 2], [0, 506, 2], [0, 2, 1], [0, 398, 2], [0, 288, 2], [0, 288, 2], [0, 288, 2], [0, 2, 1], [0, 90, 2], [0, 118, 2], [0, 119, 2], [0, 242, 2], [0, 29, 2], [0, 2, 1], [0, 102, 2], [0, 282, 2], [0, 2, 1], [0, 298, 2], [0, 139, 2], [0, 257, 2], [0, 338, 2], [0, 4, 2], [0, 2, 1]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0]]} to Sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/vpetukhov/other/NLPProjects/LearningExperiments/sbert_extending/word_weights.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vpetukhov/other/NLPProjects/LearningExperiments/sbert_extending/word_weights.ipynb#Y241sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_weighted[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mdecode(inps[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3345\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3343\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3345\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3346\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3347\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3348\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3349\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3350\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py:548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(token_ids, \u001b[39mint\u001b[39m):\n\u001b[1;32m    547\u001b[0m     token_ids \u001b[39m=\u001b[39m [token_ids]\n\u001b[0;32m--> 548\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mdecode(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[1;32m    551\u001b[0m     clean_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclean_up_tokenization(text)\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert {'input_ids': [[0, 506, 2], [0, 257, 2], [0, 462, 2], [0, 705, 2], [0, 118, 2], [0, 139, 2], [0, 2, 1], [0, 428, 2], [0, 242, 2], [0, 338, 2], [0, 571, 2], [0, 298, 2], [0, 242, 2], [0, 462, 2], [0, 462, 2], [0, 102, 2], [0, 2, 1], [0, 29, 2], [0, 90, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 102, 2], [0, 90, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 242, 2], [0, 2, 1], [0, 417, 2], [0, 118, 2], [0, 571, 2], [0, 118, 2], [0, 90, 2], [0, 102, 2], [0, 462, 2], [0, 2, 1], [0, 605, 2], [0, 139, 2], [0, 338, 2], [0, 119, 2], [0, 2, 1], [0, 282, 2], [0, 102, 2], [0, 119, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 29, 2], [0, 1343, 2], [0, 2, 1], [0, 298, 2], [0, 242, 2], [0, 462, 2], [0, 462, 2], [0, 2, 1], [0, 605, 2], [0, 102, 2], [0, 29, 2], [0, 2, 1], [0, 642, 2], [0, 102, 2], [0, 338, 2], [0, 90, 2], [0, 118, 2], [0, 438, 2], [0, 257, 2], [0, 462, 2], [0, 102, 2], [0, 338, 2], [0, 462, 2], [0, 219, 2], [0, 2, 1], [0, 705, 2], [0, 118, 2], [0, 338, 2], [0, 257, 2], [0, 462, 2], [0, 242, 2], [0, 282, 2], [0, 90, 2], [0, 2, 1], [0, 102, 2], [0, 282, 2], [0, 417, 2], [0, 2, 1], [0, 338, 2], [0, 242, 2], [0, 642, 2], [0, 462, 2], [0, 118, 2], [0, 438, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 417, 2], [0, 2, 1], [0, 118, 2], [0, 90, 2], [0, 29, 2], [0, 242, 2], [0, 462, 2], [0, 506, 2], [0, 2, 1], [0, 102, 2], [0, 90, 2], [0, 2, 1], [0, 90, 2], [0, 298, 2], [0, 242, 2], [0, 2, 1], [0, 338, 2], [0, 102, 2], [0, 90, 2], [0, 242, 2], [0, 2, 1], [0, 139, 2], [0, 506, 2], [0, 2, 1], [0, 398, 2], [0, 288, 2], [0, 288, 2], [0, 288, 2], [0, 2, 1], [0, 90, 2], [0, 118, 2], [0, 119, 2], [0, 242, 2], [0, 29, 2], [0, 2, 1], [0, 102, 2], [0, 282, 2], [0, 2, 1], [0, 298, 2], [0, 139, 2], [0, 257, 2], [0, 338, 2], [0, 4, 2], [0, 2, 1]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 0], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0]]} to Sequence"
     ]
    }
   ],
   "source": [
    "model_weighted[0].tokenizer.decode(inps[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4410],\n",
       "         [0.4084],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5261],\n",
       "         [0.4620],\n",
       "         [0.4923]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5243],\n",
       "         [0.5109],\n",
       "         [0.4907]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4686],\n",
       "         [0.4629],\n",
       "         [0.4195]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4310],\n",
       "         [0.4204],\n",
       "         [0.3855]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.4930],\n",
       "         [0.4623],\n",
       "         [0.4272]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4847],\n",
       "         [0.4442],\n",
       "         [0.4419]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4709],\n",
       "         [0.4423],\n",
       "         [0.4220]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4709],\n",
       "         [0.4423],\n",
       "         [0.4220]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4930],\n",
       "         [0.4623],\n",
       "         [0.4272]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4910],\n",
       "         [0.4762],\n",
       "         [0.4402]],\n",
       "\n",
       "        [[0.4686],\n",
       "         [0.4629],\n",
       "         [0.4195]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.5381],\n",
       "         [0.5278],\n",
       "         [0.4745]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4799],\n",
       "         [0.4806],\n",
       "         [0.4370]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.5381],\n",
       "         [0.5278],\n",
       "         [0.4745]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4709],\n",
       "         [0.4423],\n",
       "         [0.4220]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4847],\n",
       "         [0.4442],\n",
       "         [0.4419]],\n",
       "\n",
       "        [[0.4756],\n",
       "         [0.3902],\n",
       "         [0.4124]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4910],\n",
       "         [0.4762],\n",
       "         [0.4402]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4847],\n",
       "         [0.4442],\n",
       "         [0.4419]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4733],\n",
       "         [0.4621],\n",
       "         [0.4142]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4809],\n",
       "         [0.4325],\n",
       "         [0.4285]],\n",
       "\n",
       "        [[0.5261],\n",
       "         [0.4620],\n",
       "         [0.4923]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5082],\n",
       "         [0.4811],\n",
       "         [0.4549]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.5243],\n",
       "         [0.5109],\n",
       "         [0.4907]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.5261],\n",
       "         [0.4620],\n",
       "         [0.4923]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4799],\n",
       "         [0.4806],\n",
       "         [0.4370]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4799],\n",
       "         [0.4806],\n",
       "         [0.4370]],\n",
       "\n",
       "        [[0.4709],\n",
       "         [0.4423],\n",
       "         [0.4220]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4733],\n",
       "         [0.4621],\n",
       "         [0.4142]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4809],\n",
       "         [0.4325],\n",
       "         [0.4285]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4709],\n",
       "         [0.4423],\n",
       "         [0.4220]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4847],\n",
       "         [0.4442],\n",
       "         [0.4419]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.5091],\n",
       "         [0.5238],\n",
       "         [0.4647]],\n",
       "\n",
       "        [[0.4410],\n",
       "         [0.4084],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4686],\n",
       "         [0.4629],\n",
       "         [0.4195]],\n",
       "\n",
       "        [[0.4410],\n",
       "         [0.4084],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.5544],\n",
       "         [0.5371],\n",
       "         [0.5252]],\n",
       "\n",
       "        [[0.5005],\n",
       "         [0.4713],\n",
       "         [0.4388]],\n",
       "\n",
       "        [[0.5005],\n",
       "         [0.4713],\n",
       "         [0.4388]],\n",
       "\n",
       "        [[0.5005],\n",
       "         [0.4713],\n",
       "         [0.4388]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4624],\n",
       "         [0.4424],\n",
       "         [0.4127]],\n",
       "\n",
       "        [[0.5627],\n",
       "         [0.5200],\n",
       "         [0.5301]],\n",
       "\n",
       "        [[0.5381],\n",
       "         [0.5278],\n",
       "         [0.4745]],\n",
       "\n",
       "        [[0.5197],\n",
       "         [0.4970],\n",
       "         [0.4718]],\n",
       "\n",
       "        [[0.4847],\n",
       "         [0.4442],\n",
       "         [0.4419]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4844],\n",
       "         [0.4513],\n",
       "         [0.4277]],\n",
       "\n",
       "        [[0.4799],\n",
       "         [0.4806],\n",
       "         [0.4370]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]],\n",
       "\n",
       "        [[0.4905],\n",
       "         [0.4717],\n",
       "         [0.4213]],\n",
       "\n",
       "        [[0.4686],\n",
       "         [0.4629],\n",
       "         [0.4195]],\n",
       "\n",
       "        [[0.5261],\n",
       "         [0.4620],\n",
       "         [0.4923]],\n",
       "\n",
       "        [[0.4787],\n",
       "         [0.4499],\n",
       "         [0.4380]],\n",
       "\n",
       "        [[0.4617],\n",
       "         [0.4515],\n",
       "         [0.4136]],\n",
       "\n",
       "        [[0.4641],\n",
       "         [0.4520],\n",
       "         [0.5063]]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]['token_weights'][:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_weighted(X[0])['token_weights'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((1 - model_weighted(X[0])['token_weights'])).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- add mean embedding to the weighter input\n",
    "- add more layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
